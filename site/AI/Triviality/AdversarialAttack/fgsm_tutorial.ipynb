{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# FGSM - PyTorch Implementation\n",
        "\n",
        "- This is a PyTorch implementation of the Fast Gradient Sign Method (FGSM) attack from [PyTorch Official website](https://pytorch.org/tutorials/beginner/fgsm_tutorial.html?highlight=fgsm) as described in [Explaining and Harnessing Adversarial Examples](https://arxiv.org/abs/1412.6572) by Goodfellow et al. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementation\n",
        "### Import Packages and Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inputs\n",
        "\n",
        "本代码只有三个输入，分别为：\n",
        "\n",
        "-  ``epsilons`` - epsilon 是一个列表，表示对原始图像添加的扰动的大小，epsilon 越大，对原始图像的扰动越大，但是对模型的攻击效果越好，因为模型的准确率会下降。因为原始图像的像素值在 [0,1] 之间，所以 epsilon 的取值也应该在 [0,1] 之间。\n",
        "\n",
        "-  ``pretrained_model`` - 用于攻击的预训练模型。这里使用的是在 MNIST 数据集上预训练的模型，该模型的准确率为 99%。这个模型是在 PyTorch 官方的教程中提供的，可以在 _[pytorch/examples/mnist](https://github.com/pytorch/examples/tree/master/mnist)_ 找到。为了简单起见，可以在 _[这里](https://drive.google.com/file/d/1HJV2nUHJqclXQ8flKvcWmjZ-OU5DGatl/view?usp=drive_link)_ 下载预训练模型。\n",
        "\n",
        "-  ``use_cuda`` - 一个布尔值，表示是否使用 GPU。如果 use_cuda 为 True 且含有 cuda，那么就使用 GPU 进行计算，否则使用 CPU 进行计算。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "epsilons = [0, .05, .1, .15, .2, .25, .3]\n",
        "pretrained_model = \"../data/lenet_mnist_model.pt\"\n",
        "use_cuda=True\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Under Attack\n",
        "\n",
        "如上所述，受攻击的模型是来自 _[pytorch/examples/mnist](https://github.com/pytorch/examples/tree/master/mnist)_ 的 MNIST 模型。你可以训练并保存自己的 MNIST 模型，也可以下载并使用提供的模型。这里的 *Net* 定义和测试数据加载器是从 MNIST 示例中复制过来的。本节的目的是定义模型和数据加载器，然后初始化模型并加载预训练权重。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# LeNet Model definition\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(9216, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output\n",
        "\n",
        "# MNIST Test dataset and dataloader declaration\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, download=True, transform=transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.1307,), (0.3081,)),\n",
        "            ])),\n",
        "        batch_size=1, shuffle=True)\n",
        "\n",
        "# Define what device we are using\n",
        "print(\"CUDA Available: \",torch.cuda.is_available())\n",
        "device = torch.device(\"cuda\" if use_cuda and torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize the network\n",
        "model = Net().to(device)\n",
        "\n",
        "# Load the pretrained model\n",
        "model.load_state_dict(torch.load(pretrained_model, map_location=device))\n",
        "\n",
        "# Set the model in evaluation mode. In this case this is for the Dropout layers\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### FGSM Attack\n",
        "\n",
        "Now, we can define the function that creates the adversarial examples by\n",
        "perturbing the original inputs. The ``fgsm_attack`` function takes three\n",
        "inputs, *image* is the original clean image ($x$), *epsilon* is\n",
        "the pixel-wise perturbation amount ($\\epsilon$), and *data_grad*\n",
        "is gradient of the loss w.r.t the input image\n",
        "($\\nabla_{x} J(\\mathbf{\\theta}, \\mathbf{x}, y)$). The function\n",
        "then creates perturbed image as\n",
        "\n",
        "\\begin{align}perturbed\\_image = image + epsilon*sign(data\\_grad) = x + \\epsilon * sign(\\nabla_{x} J(\\mathbf{\\theta}, \\mathbf{x}, y))\\end{align}\n",
        "\n",
        "Finally, in order to maintain the original range of the data, the\n",
        "perturbed image is clipped to range $[0,1]$.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# FGSM attack code\n",
        "def fgsm_attack(image, epsilon, data_grad):\n",
        "    # 使用sign（符号）函数，将对 x 求了偏导的梯度进行符号化\n",
        "    sign_data_grad = data_grad.sign()\n",
        "    # 通过epsilon生成对抗样本\n",
        "    perturbed_image = image + epsilon*sign_data_grad\n",
        "    # 做一个剪裁的工作，将 torch.clamp 内部数值保持在 [0,1]，防止image越界\n",
        "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
        "    # Return the perturbed image\n",
        "    return perturbed_image\n",
        "\n",
        "# restores the tensors to their original scale\n",
        "def denorm(batch, mean=[0.1307], std=[0.3081]):\n",
        "    \"\"\"\n",
        "    Convert a batch of tensors to their original scale.\n",
        "\n",
        "    Args:\n",
        "        batch (torch.Tensor): Batch of normalized tensors.\n",
        "        mean (torch.Tensor or list): Mean used for normalization.\n",
        "        std (torch.Tensor or list): Standard deviation used for normalization.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: batch of tensors without normalization applied to them.\n",
        "    \"\"\"\n",
        "    if isinstance(mean, list):\n",
        "        mean = torch.tensor(mean).to(device)\n",
        "    if isinstance(std, list):\n",
        "        std = torch.tensor(std).to(device)\n",
        "\n",
        "    return batch * std.view(1, -1, 1, 1) + mean.view(1, -1, 1, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing Function\n",
        "\n",
        "本教程的核心结果来自 ``test`` 函数。每次调用此测试函数都会对 MNIST 测试集执行完整的测试步骤，并报告最终的准确率。但是，请注意，此函数还需要一个 *epsilon* 输入。这是因为 ``test`` 函数报告了受到 $\\epsilon$ 强度的对抗者攻击的模型的准确性。更具体地说，对于测试集中的每个样本，该函数计算损失相对于输入数据的梯度 ($data\\_grad$)，使用 ``fgsm_attack`` 创建扰动图像 ($perturbed\\_data$)，然后检查扰动的图像是否是对抗性的。除了测试模型的准确性之外，该函数还保存并返回一些成功的对抗性示例，以便稍后进行可视化。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def test( model, device, test_loader, epsilon ):\n",
        "\n",
        "    # Accuracy counter\n",
        "    correct = 0\n",
        "    adv_examples = []\n",
        "\n",
        "    # Loop over all examples in test set\n",
        "    for data, target in test_loader:\n",
        "\n",
        "        # Send the data and label to the device\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        # Set requires_grad attribute of tensor. Important for Attack\n",
        "        data.requires_grad = True\n",
        "\n",
        "        # Forward pass the data through the model\n",
        "        output = model(data)\n",
        "        init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "\n",
        "        # If the initial prediction is wrong, don't bother attacking, just move on\n",
        "        if init_pred.item() != target.item():\n",
        "            continue\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = F.nll_loss(output, target)\n",
        "\n",
        "        # Zero all existing gradients\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Calculate gradients of model in backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Collect ``datagrad``\n",
        "        data_grad = data.grad.data\n",
        "\n",
        "        # Restore the data to its original scale\n",
        "        data_denorm = denorm(data)\n",
        "\n",
        "        # Call FGSM Attack\n",
        "        perturbed_data = fgsm_attack(data_denorm, epsilon, data_grad)\n",
        "\n",
        "        # Reapply normalization\n",
        "        perturbed_data_normalized = transforms.Normalize((0.1307,), (0.3081,))(perturbed_data)\n",
        "\n",
        "        # Re-classify the perturbed image\n",
        "        output = model(perturbed_data_normalized)\n",
        "\n",
        "        # Check for success\n",
        "        final_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "        if final_pred.item() == target.item():\n",
        "            correct += 1\n",
        "            # Special case for saving 0 epsilon examples\n",
        "            if epsilon == 0 and len(adv_examples) < 5:\n",
        "                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
        "                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n",
        "        else:\n",
        "            # Save some adv examples for visualization later\n",
        "            if len(adv_examples) < 5:\n",
        "                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
        "                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n",
        "\n",
        "    # Calculate final accuracy for this epsilon\n",
        "    final_acc = correct/float(len(test_loader))\n",
        "    print(f\"Epsilon: {epsilon}\\tTest Accuracy = {correct} / {len(test_loader)} = {final_acc}\")\n",
        "\n",
        "    # Return the accuracy and an adversarial example\n",
        "    return final_acc, adv_examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run Attack\n",
        "\n",
        "在这里，我们为 *epsilons* 输入中的每个 epsilon 值运行一个完整的测试步骤。对于每个 epsilon，我们还保存最终的准确性和一些成功的对抗性示例，以便在接下来的部分进行绘制。请注意，打印的准确性随着 epsilon 值的增加而降低。此外，注意 $\\epsilon=0$ 的情况表示原始测试准确性，没有攻击。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "accuracies = []\n",
        "examples = []\n",
        "\n",
        "# Run test for each epsilon\n",
        "for eps in epsilons:\n",
        "    acc, ex = test(model, device, test_loader, eps)\n",
        "    accuracies.append(acc)\n",
        "    examples.append(ex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results\n",
        "\n",
        "### Accuracy vs Epsilon\n",
        "\n",
        "The first result is the accuracy versus epsilon plot. As alluded to\n",
        "earlier, as epsilon increases we expect the test accuracy to decrease.\n",
        "This is because larger epsilons mean we take a larger step in the\n",
        "direction that will maximize the loss. Notice the trend in the curve is\n",
        "not linear even though the epsilon values are linearly spaced. For\n",
        "example, the accuracy at $\\epsilon=0.05$ is only about 4% lower\n",
        "than $\\epsilon=0$, but the accuracy at $\\epsilon=0.2$ is 25%\n",
        "lower than $\\epsilon=0.15$. Also, notice the accuracy of the model\n",
        "hits random accuracy for a 10-class classifier between\n",
        "$\\epsilon=0.25$ and $\\epsilon=0.3$.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(5,5))\n",
        "plt.plot(epsilons, accuracies, \"*-\")\n",
        "plt.yticks(np.arange(0, 1.1, step=0.1))\n",
        "plt.xticks(np.arange(0, .35, step=0.05))\n",
        "plt.title(\"Accuracy vs Epsilon\")\n",
        "plt.xlabel(\"Epsilon\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sample Adversarial Examples\n",
        "\n",
        "Remember the idea of no free lunch? In this case, as epsilon increases\n",
        "the test accuracy decreases **BUT** the perturbations become more easily\n",
        "perceptible. In reality, there is a tradeoff between accuracy\n",
        "degradation and perceptibility that an attacker must consider. Here, we\n",
        "show some examples of successful adversarial examples at each epsilon\n",
        "value. Each row of the plot shows a different epsilon value. The first\n",
        "row is the $\\epsilon=0$ examples which represent the original\n",
        "“clean” images with no perturbation. The title of each image shows the\n",
        "“original classification -> adversarial classification.” Notice, the\n",
        "perturbations start to become evident at $\\epsilon=0.15$ and are\n",
        "quite evident at $\\epsilon=0.3$. However, in all cases humans are\n",
        "still capable of identifying the correct class despite the added noise.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Plot several examples of adversarial samples at each epsilon\n",
        "cnt = 0\n",
        "plt.figure(figsize=(8,10))\n",
        "for i in range(len(epsilons)):\n",
        "    for j in range(len(examples[i])):\n",
        "        cnt += 1\n",
        "        plt.subplot(len(epsilons),len(examples[0]),cnt)\n",
        "        plt.xticks([], [])\n",
        "        plt.yticks([], [])\n",
        "        if j == 0:\n",
        "            plt.ylabel(f\"Eps: {epsilons[i]}\", fontsize=14)\n",
        "        orig,adv,ex = examples[i][j]\n",
        "        plt.title(f\"{orig} -> {adv}\")\n",
        "        plt.imshow(ex, cmap=\"gray\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Where to go next?\n",
        "\n",
        "Hopefully this tutorial gives some insight into the topic of adversarial\n",
        "machine learning. There are many potential directions to go from here.\n",
        "This attack represents the very beginning of adversarial attack research\n",
        "and since there have been many subsequent ideas for how to attack and\n",
        "defend ML models from an adversary. In fact, at NIPS 2017 there was an\n",
        "adversarial attack and defense competition and many of the methods used\n",
        "in the competition are described in this paper: [Adversarial Attacks and\n",
        "Defences Competition](https://arxiv.org/pdf/1804.00097.pdf)_. The work\n",
        "on defense also leads into the idea of making machine learning models\n",
        "more *robust* in general, to both naturally perturbed and adversarially\n",
        "crafted inputs.\n",
        "\n",
        "Another direction to go is adversarial attacks and defense in different\n",
        "domains. Adversarial research is not limited to the image domain, check\n",
        "out [this](https://arxiv.org/pdf/1801.01944.pdf)_ attack on\n",
        "speech-to-text models. But perhaps the best way to learn more about\n",
        "adversarial machine learning is to get your hands dirty. Try to\n",
        "implement a different attack from the NIPS 2017 competition, and see how\n",
        "it differs from FGSM. Then, try to defend the model from your own\n",
        "attacks.\n",
        "\n",
        "A further direction to go, depending on available resources, is to modify\n",
        "the code to support processing work in batch, in parallel, and or distributed\n",
        "vs working on one attack at a time in the above for each ``epsilon test()`` loop.\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
